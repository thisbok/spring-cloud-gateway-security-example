input {
	 redis {
                host => "redis"
                codec => "json"
                data_type => "list"
                key => "transactions"
        }


        # Kafka input for log events
        kafka {
                bootstrap_servers => "kafka:29092"
                topics => ["logs", "events", "metrics"]
                group_id => "logstash_consumer"
                codec => "json"
                type => "kafka"
                consumer_threads => 3
        }
}

# input {
# 	 redis {
#                 host => "redis"
#                 codec => "json"
#                 data_type => "list"
#                 key => "log-event"
#         }
# }

## Add your filters / logstash plugins configuration here
filter {
        if [type] == "kafka" {
                # Kafka 데이터 처리
                mutate {
                        add_field => { "[@metadata][target_index]" => "kafka-logs-%{+YYYY.MM.dd}" }
                }

                # 타임스탬프 처리
                if [timestamp] {
                        date {
                                match => [ "timestamp", "ISO8601", "UNIX", "UNIX_MS" ]
                                target => "@timestamp"
                        }
                }

                # 로그 레벨 정규화
                if [level] {
                        mutate {
                                uppercase => [ "level" ]
                        }
                }

                # 서비스명 추가
                if ![service] and [logger] {
                        mutate {
                                add_field => { "service" => "%{logger}" }
                        }
                }
        } else {
                # 기존 Redis 트랜잭션 데이터 처리
                # (선택적) 데이터 필터링 및 전처리를 수행할 수 있습니다.
                # if [service] == "" or [_docId] == "" or [event] ==  "" {
                #         drop {}
                # }

                mutate {
                        add_field => { "[@metadata][target_index]" => "%{_index}" }

                        # docId 를 생성받을경우 @metadata 속성을 이용하여 처리
                        # doc_id 는 되도록이면 자동생성하는게 성능에 도움됨
                        # add_field => { "[@metadata][doc_id]" => "%{_docId}%{}" }
                }

                mutate {
                        remove_field => ["[event][original]","_index"]
                }
        }
}

output {
        if [type] == "kafka" {
                # Kafka 데이터 출력
                elasticsearch {
                        hosts => "elasticsearch:9200"
                        user => "logstash_internal"
                        password => "${LOGSTASH_INTERNAL_PASSWORD}"
                        index => "%{[@metadata][target_index]}"
                }
        } else {
                # 기존 트랜잭션 데이터 출력
                elasticsearch {
                        hosts => "elasticsearch:9200"
                        user => "logstash_internal"
                        password => "${LOGSTASH_INTERNAL_PASSWORD}"
                        index => "%{[@metadata][target_index]}"
                        # doc_as_upsert => true
                        # document_id => "%{[@metadata][doc_id]}"
                        # template => "/usr/share/logstash/config/log-template.json"
                        # manage_template => false
                }
        }
}